{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9976672,"sourceType":"datasetVersion","datasetId":6138561},{"sourceId":9979163,"sourceType":"datasetVersion","datasetId":6140355}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch transformers pandas datasets rouge-score evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:16:16.869180Z","iopub.execute_input":"2024-11-22T08:16:16.869861Z","iopub.status.idle":"2024-11-22T08:16:28.357733Z","shell.execute_reply.started":"2024-11-22T08:16:16.869826Z","shell.execute_reply":"2024-11-22T08:16:28.356873Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ba1f6c130faa0efe7cbc5e0e17bb6d095be194b528eef2cb6de85bb45457ad93\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score, evaluate\nSuccessfully installed evaluate-0.4.3 rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom rouge_score import rouge_scorer\n\n# Load preprocessed train and validation data\ntrain_file_path = \"/kaggle/input/input-data/train_600.csv\"\nval_file_path = \"/kaggle/input/input-data/val_600.csv\"\n\ntrain_df = pd.read_csv(train_file_path)\nval_df = pd.read_csv(val_file_path)\n\n# Ensure all values are strings and handle missing data\ndef preprocess_dataframe(df):\n    df['Text'] = df['Text'].astype(str)\n    df['Summary'] = df['Summary'].astype(str)\n    return df\n\ntrain_df = preprocess_dataframe(train_df)\nval_df = preprocess_dataframe(val_df)\n\n# Initialize T5 tokenizer and model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:17:10.447771Z","iopub.execute_input":"2024-11-22T08:17:10.447985Z","iopub.status.idle":"2024-11-22T08:17:10.928899Z","shell.execute_reply.started":"2024-11-22T08:17:10.447963Z","shell.execute_reply":"2024-11-22T08:17:10.928143Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class T5PointerGenerator(nn.Module):\n    def __init__(self, model_name):\n        super(T5PointerGenerator, self).__init__()\n        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        if labels is not None:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels,\n                output_hidden_states=True\n            )\n        else:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_attentions=True,\n                output_hidden_states=True\n            )\n        return outputs\n\n    def generate(self, *args, **kwargs):\n        \"\"\"\n        Forward the generate call to the underlying T5 model's generate method.\n        \"\"\"\n        return self.model.generate(*args, **kwargs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:17:10.939497Z","iopub.execute_input":"2024-11-22T08:17:10.939860Z","iopub.status.idle":"2024-11-22T08:17:10.951062Z","shell.execute_reply.started":"2024-11-22T08:17:10.939820Z","shell.execute_reply":"2024-11-22T08:17:10.950350Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class SummarizationDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_input_length=700, max_target_length=200):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data.iloc[idx]['Text']\n        summary = self.data.iloc[idx]['Summary']\n\n        text_encoding = self.tokenizer(\n            text,\n            max_length=self.max_input_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        summary_encoding = self.tokenizer(\n            summary,\n            max_length=self.max_target_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        input_ids = text_encoding[\"input_ids\"].squeeze()\n        attention_mask = text_encoding[\"attention_mask\"].squeeze()\n        labels = summary_encoding[\"input_ids\"].squeeze()\n\n        return input_ids, attention_mask, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:17:15.165391Z","iopub.execute_input":"2024-11-22T08:17:15.166108Z","iopub.status.idle":"2024-11-22T08:17:15.172259Z","shell.execute_reply.started":"2024-11-22T08:17:15.166064Z","shell.execute_reply":"2024-11-22T08:17:15.171417Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model_name = \"t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5PointerGenerator(model_name=model_name).to(device)\n\ntrain_dataset = SummarizationDataset(train_df, tokenizer)\nval_dataset = SummarizationDataset(val_df, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=4)\n\n# Training setup\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T04:52:00.563042Z","iopub.execute_input":"2024-11-22T04:52:00.563818Z","iopub.status.idle":"2024-11-22T04:52:05.228973Z","shell.execute_reply.started":"2024-11-22T04:52:00.563785Z","shell.execute_reply":"2024-11-22T04:52:05.228088Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db58e9734940428aa5be8569e2bbe182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20b151fe36034d2c8e7270ba5c65e65c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2cb7e46c0a54080b13e6c42ef06a702"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab1d2e48d57f464a842005def5317c15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c5690a5984340e7af94a83a6ea6b2f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a61fe57582c146f5a1952b01cab26374"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n\n    for input_ids, attention_mask, labels in dataloader:\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        outputs = model(input_ids, attention_mask, labels=labels)\n        logits = outputs.logits\n\n        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:17:58.350352Z","iopub.execute_input":"2024-11-22T08:17:58.351183Z","iopub.status.idle":"2024-11-22T08:17:58.356624Z","shell.execute_reply.started":"2024-11-22T08:17:58.351148Z","shell.execute_reply":"2024-11-22T08:17:58.355736Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def validate_model(model, dataloader, tokenizer, device):\n    model.eval()\n    predictions = []\n    references = []\n\n    with torch.no_grad():\n        for input_ids, attention_mask, labels in dataloader:\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n            # Generate predictions\n            generated_ids = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=200,  # Adjust based on your desired output length\n                num_beams=4,     # Beam search for better results\n                early_stopping=True\n            )\n\n            # Decode predictions and references\n            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n            decoded_refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n            predictions.extend(decoded_preds)\n            references.extend(decoded_refs)\n\n    return predictions, references\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:18:01.481213Z","iopub.execute_input":"2024-11-22T08:18:01.481518Z","iopub.status.idle":"2024-11-22T08:18:01.487285Z","shell.execute_reply.started":"2024-11-22T08:18:01.481492Z","shell.execute_reply":"2024-11-22T08:18:01.486395Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def calculate_rouge(predictions, references):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    total_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n\n    for pred, ref in zip(predictions, references):\n        scores = scorer.score(ref, pred)\n        total_scores['rouge1'] += scores['rouge1'].fmeasure\n        total_scores['rouge2'] += scores['rouge2'].fmeasure\n        total_scores['rougeL'] += scores['rougeL'].fmeasure\n\n    for key in total_scores:\n        total_scores[key] /= len(predictions)\n\n    return total_scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:18:04.307998Z","iopub.execute_input":"2024-11-22T08:18:04.308786Z","iopub.status.idle":"2024-11-22T08:18:04.313882Z","shell.execute_reply.started":"2024-11-22T08:18:04.308755Z","shell.execute_reply":"2024-11-22T08:18:04.312925Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"num_epochs = 5\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n\npredictions, references = validate_model(model, val_loader, tokenizer, device)\nrouge_scores = calculate_rouge(predictions, references)\n\nprint(f\"Validation ROUGE Scores: {rouge_scores}\")\n\n# Save the trained model\nmodel_save_path = \"/kaggle/working/t5_pointer_generator_model.pth\"\nmodel.tokenizer_save_path = \"/kaggle/working/t5_tokenizer\"\n\nmodel.model.save_pretrained(model_save_path)\ntokenizer.save_pretrained(model.tokenizer_save_path)\n\nprint(f\"Model saved to {model_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T04:52:33.997901Z","iopub.execute_input":"2024-11-22T04:52:33.998597Z","iopub.status.idle":"2024-11-22T05:43:41.195800Z","shell.execute_reply.started":"2024-11-22T04:52:33.998564Z","shell.execute_reply":"2024-11-22T05:43:41.194832Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 3.0435\nEpoch 3/5, Loss: 2.5955\nEpoch 4/5, Loss: 2.5078\nEpoch 5/5, Loss: 2.4379\nValidation ROUGE Scores: {'rouge1': 0.4458189776248656, 'rouge2': 0.1842469108144673, 'rougeL': 0.27792086658622395}\nModel saved to /kaggle/working/t5_pointer_generator_model.pth\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"train_file_path = \"/kaggle/input/input-300/train_300.csv\"\nval_file_path = \"/kaggle/input/input-300/val_300.csv\"\n\ntrain_df = pd.read_csv(train_file_path)\nval_df = pd.read_csv(val_file_path)\n\n# Ensure all values are strings and handle missing data\ndef preprocess_dataframe(df):\n    df['Text'] = df['Text'].astype(str)\n    df['Summary'] = df['Summary'].astype(str)\n    return df\n\ntrain_df = preprocess_dataframe(train_df)\nval_df = preprocess_dataframe(val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:17:27.987422Z","iopub.execute_input":"2024-11-22T08:17:27.988079Z","iopub.status.idle":"2024-11-22T08:17:28.715859Z","shell.execute_reply.started":"2024-11-22T08:17:27.988043Z","shell.execute_reply":"2024-11-22T08:17:28.715127Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model_name = \"t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5PointerGenerator(model_name=model_name).to(device)\n\ntrain_dataset = SummarizationDataset(train_df, tokenizer)\nval_dataset = SummarizationDataset(val_df, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=4)\n\n# Training setup\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:17:43.534761Z","iopub.execute_input":"2024-11-22T08:17:43.535584Z","iopub.status.idle":"2024-11-22T08:17:47.669409Z","shell.execute_reply.started":"2024-11-22T08:17:43.535552Z","shell.execute_reply":"2024-11-22T08:17:47.668323Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"842b5629230446e59e9f56fc999bb760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b97eaa65b82a4a0496bd76b2d420a0a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e9f43ae65844fdb715edba4b022555"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"548bc73bf3744bc99fcb90b34782ab5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8838b87d19954f5cb02df8a670645eab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc509b0c17de47fb83eb0d9c9f112086"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"num_epochs = 5\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n\npredictions, references = validate_model(model, val_loader, tokenizer, device)\nrouge_scores = calculate_rouge(predictions, references)\n\nprint(f\"Validation ROUGE Scores: {rouge_scores}\")\n\n# Save the trained model\nmodel_save_path = \"/kaggle/working/t5_pointer_generator_model_300.pth\"\nmodel.tokenizer_save_path = \"/kaggle/working/t5_tokenizer_300\"\n\nmodel.model.save_pretrained(model_save_path)\ntokenizer.save_pretrained(model.tokenizer_save_path)\n\nprint(f\"Model saved to {model_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:18:10.924814Z","iopub.execute_input":"2024-11-22T08:18:10.925546Z","iopub.status.idle":"2024-11-22T09:11:06.987257Z","shell.execute_reply.started":"2024-11-22T08:18:10.925511Z","shell.execute_reply":"2024-11-22T09:11:06.986178Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 3.4305\nEpoch 2/5, Loss: 3.1189\nEpoch 3/5, Loss: 2.9836\nEpoch 5/5, Loss: 2.8197\nValidation ROUGE Scores: {'rouge1': 0.41712182652945134, 'rouge2': 0.15597624896170764, 'rougeL': 0.2572208205406291}\nModel saved to /kaggle/working/t5_pointer_generator_model_300.pth\n","output_type":"stream"}],"execution_count":12}]}