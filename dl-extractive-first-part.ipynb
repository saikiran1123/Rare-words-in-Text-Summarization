{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load dataset\ndataset = load_dataset('ninadn/indian-legal')\ntrain_df = pd.DataFrame(dataset['train'])\n\n# Clean dataset\ndef clean_dataframe(df):\n    df = df.dropna(subset=['Text', 'Summary'])\n    df = df[(df['Text'].str.strip() != '') & (df['Summary'].str.strip() != '')]\n    return df\n\ntrain_df = clean_dataframe(train_df)\n\n# Split into 90% train and 10% validation\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\ntrain_df.reset_index(drop=True, inplace=True)\nval_df.reset_index(drop=True, inplace=True)\n\n# Optional: Remove stopwords\ndef remove_stopwords(text):\n    stop_words = set(['the', 'is', 'in', 'and', 'to', 'with', 'a', 'of', 'for', 'on', 'that', 'as', 'at', 'by', 'it', 'this', 'an', 'are', 'or'])\n    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n\ntrain_df['Text'] = train_df['Text'].apply(remove_stopwords)\nval_df['Text'] = val_df['Text'].apply(remove_stopwords)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:46:37.277750Z","iopub.execute_input":"2024-11-21T15:46:37.278187Z","iopub.status.idle":"2024-11-21T15:46:44.564444Z","shell.execute_reply.started":"2024-11-21T15:46:37.278161Z","shell.execute_reply":"2024-11-21T15:46:44.563704Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class LegalDataset(Dataset):\n    def __init__(self, data: pd.DataFrame, tokenizer, max_len=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data.iloc[idx]['Text']\n        summary = self.data.iloc[idx]['Summary']\n\n        text_encodings = self.tokenizer(\n            text,\n            return_tensors='pt',\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len\n        )\n\n        summary_encodings = self.tokenizer(\n            summary,\n            return_tensors='pt',\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len\n        )\n\n        return text_encodings.input_ids.squeeze(), text_encodings.attention_mask.squeeze(), summary_encodings.input_ids.squeeze()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:46:44.566215Z","iopub.execute_input":"2024-11-21T15:46:44.567026Z","iopub.status.idle":"2024-11-21T15:46:44.573071Z","shell.execute_reply.started":"2024-11-21T15:46:44.566981Z","shell.execute_reply":"2024-11-21T15:46:44.572067Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class PointerGenerator(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(PointerGenerator, self).__init__()\n        self.encoder = BertModel.from_pretrained('bert-base-uncased')\n        self.decoder = nn.LSTM(embedding_dim, hidden_dim)\n        self.pointer_layer = nn.Linear(hidden_dim, vocab_size)\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n    def forward(self, input_ids, attention_mask):\n        encoder_outputs = self.encoder(input_ids, attention_mask=attention_mask)[0]\n        decoder_outputs, _ = self.decoder(encoder_outputs)\n        final_logits = self.pointer_layer(decoder_outputs)\n        return final_logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:46:44.575221Z","iopub.execute_input":"2024-11-21T15:46:44.575529Z","iopub.status.idle":"2024-11-21T15:46:44.587614Z","shell.execute_reply.started":"2024-11-21T15:46:44.575502Z","shell.execute_reply":"2024-11-21T15:46:44.586746Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Initialize tokenizer and datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = LegalDataset(train_df, tokenizer)\nval_dataset = LegalDataset(val_df, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n# Model, optimizer, and loss\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = PointerGenerator(vocab_size=tokenizer.vocab_size, embedding_dim=768, hidden_dim=768).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\n# Train function\ndef train_epoch(model, data_loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for input_ids, attention_mask, labels in data_loader:\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs.view(-1, tokenizer.vocab_size), labels.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    return total_loss / len(data_loader)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:46:44.588765Z","iopub.execute_input":"2024-11-21T15:46:44.589203Z","iopub.status.idle":"2024-11-21T15:46:45.833990Z","shell.execute_reply.started":"2024-11-21T15:46:44.589163Z","shell.execute_reply":"2024-11-21T15:46:45.833284Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"epochs = 5\nfor epoch in range(epochs):\n    loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:46:45.835186Z","iopub.execute_input":"2024-11-21T15:46:45.835869Z","iopub.status.idle":"2024-11-21T15:54:49.352604Z","shell.execute_reply.started":"2024-11-21T15:46:45.835826Z","shell.execute_reply":"2024-11-21T15:54:49.351333Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 3\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[5], line 26\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 26\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"def save_model(model, tokenizer, model_path, tokenizer_path):\n    torch.save(model.state_dict(), model_path)\n    tokenizer.save_pretrained(tokenizer_path)\n\ndef load_model(model, tokenizer, model_path, tokenizer_path):\n    model.load_state_dict(torch.load(model_path))\n    tokenizer.from_pretrained(tokenizer_path)\n    return model, tokenizer\n\n# Paths for saving and loading\nmodel_path = \"/kaggle/working/pointer_generator_model.pth\"\ntokenizer_path = \"/kaggle/working/tokenizer\"\n\n# Save model and tokenizer\nsave_model(model, tokenizer, model_path, tokenizer_path)\n\n# Load saved model and tokenizer\nmodel, tokenizer = load_model(model, tokenizer, model_path, tokenizer_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:54:58.065573Z","iopub.execute_input":"2024-11-21T15:54:58.065965Z","iopub.status.idle":"2024-11-21T15:55:00.362854Z","shell.execute_reply.started":"2024-11-21T15:54:58.065933Z","shell.execute_reply":"2024-11-21T15:55:00.362068Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_227/2582030402.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model, tokenizer = load_model(model, tokenizer, model_path, tokenizer_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:55:19.583661Z","iopub.execute_input":"2024-11-21T15:55:19.584019Z","iopub.status.idle":"2024-11-21T15:55:20.108357Z","shell.execute_reply.started":"2024-11-21T15:55:19.583987Z","shell.execute_reply":"2024-11-21T15:55:20.107676Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_227/2582030402.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def generate_ngrams(text, n):\n    tokens = nltk.word_tokenize(text)\n    if len(tokens) < n:\n        return Counter()  # Return an empty counter if not enough tokens\n    ngrams_list = list(ngrams(tokens, n))\n    ngram_counts = Counter(ngrams_list)\n    total_ngrams = sum(ngram_counts.values())\n    ngram_scores = {ngram: total_ngrams / freq for ngram, freq in ngram_counts.items()}\n    return ngram_scores\n\n\ndef score_sentence_with_ngrams(sentence, ngram_scores, n=3):\n    tokens = nltk.word_tokenize(sentence)\n    if len(tokens) < n:\n        # If not enough tokens for n-grams, assign a default score of 0\n        return 0\n\n    sentence_ngrams = list(ngrams(tokens, n))\n    score = sum(ngram_scores.get(ngram, 0) for ngram in sentence_ngrams)\n    return score\n\n\ndef generate_summary_with_model(text, model, tokenizer, ngram_scores, max_tokens=None, n=3):\n    sentences = nltk.sent_tokenize(text)\n    sentences= sentences[:40]\n    inputs = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n        sentence_embeddings = outputs[:, 0, :].cpu()\n\n    torch.cuda.empty_cache()\n    sentence_scores = []\n    for idx, sentence in enumerate(sentences):\n        ngram_score = score_sentence_with_ngrams(sentence, ngram_scores, n)\n        sentence_score = ngram_score * torch.norm(sentence_embeddings[idx])\n        sentence_scores.append((sentence, sentence_score))\n\n    sentence_scores = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n\n    # If max_tokens is None, include all sentences\n    if max_tokens is None:\n        return \" \".join([sent for sent, _ in sentence_scores])\n\n    summary_tokens, selected_sentences = 0, []\n    for sent, _ in sentence_scores:\n        tokens = len(tokenizer.tokenize(sent))\n        if summary_tokens + tokens > max_tokens:\n            break\n        selected_sentences.append(sent)\n        summary_tokens += tokens\n\n    return \" \".join(selected_sentences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:55:25.302462Z","iopub.execute_input":"2024-11-21T15:55:25.302813Z","iopub.status.idle":"2024-11-21T15:55:25.312287Z","shell.execute_reply.started":"2024-11-21T15:55:25.302782Z","shell.execute_reply":"2024-11-21T15:55:25.311364Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Ensure you are using GPU 1\ndevice = torch.device(\"cuda:1\" if torch.cuda.device_count() > 1 else \"cuda:0\")\nprint(f\"Using device: {device}\")\n\n# Move model to GPU 1\nmodel = model.to(device)\n\ndef evaluate_model_on_validation(val_df, model, tokenizer, max_tokens_list):\n    total_rouge_scores = {tokens: {'rouge1': 0, 'rouge2': 0, 'rougeL': 0} for tokens in max_tokens_list}\n\n    for idx in range(len(val_df)):\n        text = val_df.iloc[idx]['Text']\n        reference_summary = val_df.iloc[idx]['Summary']\n        ngram_scores = generate_ngrams(text, 3)\n\n        # Generate summaries for each token limit\n        for max_tokens in max_tokens_list:\n            generated_summary = generate_summary_with_model(text, model, tokenizer, ngram_scores, max_tokens)\n\n            scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n            rouge_scores = scorer.score(reference_summary, generated_summary)\n\n            total_rouge_scores[max_tokens]['rouge1'] += rouge_scores['rouge1'].fmeasure\n            total_rouge_scores[max_tokens]['rouge2'] += rouge_scores['rouge2'].fmeasure\n            total_rouge_scores[max_tokens]['rougeL'] += rouge_scores['rougeL'].fmeasure\n\n    # Normalize scores\n    for max_tokens in total_rouge_scores:\n        for metric in total_rouge_scores[max_tokens]:\n            total_rouge_scores[max_tokens][metric] /= len(val_df)\n\n    return total_rouge_scores\n\n\n\nmax_tokens_list = [300, 600]\n\n# Evaluate and print results\nrouge_scores = evaluate_model_on_validation(val_df, model, tokenizer, max_tokens_list)\n\nfor max_tokens, scores in rouge_scores.items():\n    print(f\"\\nResults for summaries up to {max_tokens} tokens:\")\n    print(f\"ROUGE-1: {scores['rouge1']}\")\n    print(f\"ROUGE-2: {scores['rouge2']}\")\n    print(f\"ROUGE-L: {scores['rougeL']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:55:33.263976Z","iopub.execute_input":"2024-11-21T15:55:33.264713Z","iopub.status.idle":"2024-11-21T16:04:24.863332Z","shell.execute_reply.started":"2024-11-21T15:55:33.264679Z","shell.execute_reply":"2024-11-21T16:04:24.862328Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda:1\n\nResults for summaries up to 300 tokens:\nROUGE-1: 0.2543399457127066\nROUGE-2: 0.07534923595466274\nROUGE-L: 0.12287232867720368\n\nResults for summaries up to 600 tokens:\nROUGE-1: 0.33299165679905457\nROUGE-2: 0.10284371873330596\nROUGE-L: 0.1412258131158518\n","output_type":"stream"}],"execution_count":10}]}